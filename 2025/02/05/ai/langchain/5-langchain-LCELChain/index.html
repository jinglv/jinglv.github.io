<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jinglv.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="学习LangChain学习笔记第五讲">
<meta property="og:type" content="article">
<meta property="og:title" content="LangChain LCEL表达式语言">
<meta property="og:url" content="https://jinglv.github.io/2025/02/05/ai/langchain/5-langchain-LCELChain/index.html">
<meta property="og:site_name" content="Jean&#39;s Blog">
<meta property="og:description" content="学习LangChain学习笔记第五讲">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jing-images.oss-cn-beijing.aliyuncs.com/img/image-20250904091914451.png">
<meta property="og:image" content="https://jing-images.oss-cn-beijing.aliyuncs.com/img/image-20250904094128108.png">
<meta property="og:image" content="https://jing-images.oss-cn-beijing.aliyuncs.com/img/image-20250904094645701.png">
<meta property="article:published_time" content="2025-02-04T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-08T05:30:00.000Z">
<meta property="article:author" content="Jean Lv">
<meta property="article:tag" content="LangChain">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jing-images.oss-cn-beijing.aliyuncs.com/img/image-20250904091914451.png">

<link rel="canonical" href="https://jinglv.github.io/2025/02/05/ai/langchain/5-langchain-LCELChain/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>LangChain LCEL表达式语言 | Jean's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Jean's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Jean's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">一个专注软件测试开发技术的个人博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jinglv.github.io/2025/02/05/ai/langchain/5-langchain-LCELChain/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jean Lv">
      <meta itemprop="description" content="涉猎的主要编程语言Java、Python, 领域软件测试、AI测试开发相关">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jean's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LangChain LCEL表达式语言
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-05 00:00:00" itemprop="dateCreated datePublished" datetime="2025-02-05T00:00:00+08:00">2025-02-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-08 13:30:00" itemprop="dateModified" datetime="2025-09-08T13:30:00+08:00">2025-09-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LangChain/" itemprop="url" rel="index"><span itemprop="name">LangChain</span></a>
                </span>
            </span>

          
            <div class="post-description">学习LangChain学习笔记第五讲</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Runnable介绍"><a href="#Runnable介绍" class="headerlink" title="Runnable介绍"></a>Runnable介绍</h1><h2 id="什么是Runnable"><a href="#什么是Runnable" class="headerlink" title="什么是Runnable"></a>什么是Runnable</h2><p><code>Runnable</code> 是 LangChain 0.3 中的核心接口，代表<strong>一个可执行的组件</strong>。它可以被调用（同步或异步），也可以与其他 <code>Runnable</code> 对象组合构建更复杂的链。</p>
<p> Runnable接口是Langchain的核心组件通信标准，通过统一输入输出格式及调用方式，降低组件间耦合度，支持同步异步操作，简化工作流构建，使不同组件能无缝连接，并提供运行时配置功能以增强灵活性和可扩展性。</p>
<p><img src="https://jing-images.oss-cn-beijing.aliyuncs.com/img/image-20250904091914451.png" alt="image-20250904091914451" style="zoom:50%;" /></p>
<ul>
<li>Runnable是LangChain的核心接口，提供统一的调用方式，使不同组件可以无缝连接</li>
<li>简化了复杂AI工作流的构建过程</li>
<li>支持同步和异步操作</li>
<li>可通过管道操作符 ｜ 连接多个组件</li>
</ul>
<p>注意：并不是所有的组件都支持所有的统一的调用方法，尤其是异步事件</p>
<p>Runnable对组件的输入输出要求：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Component</th>
<th>Input Type</th>
<th>Output Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prompt</td>
<td>dictionary</td>
<td>PromptValue</td>
</tr>
<tr>
<td>ChatModel</td>
<td>a string, list of chat messages or PromptValue</td>
<td>ChatMessage</td>
</tr>
<tr>
<td>LLM</td>
<td>a string, list of chat messages or PromptValue</td>
<td>String</td>
</tr>
<tr>
<td>OutputParser</td>
<td>the output of an LLM or ChatModel</td>
<td>Depends on the parser</td>
</tr>
<tr>
<td>Retriever</td>
<td>a string</td>
<td>List of Documents</td>
</tr>
<tr>
<td>Tool</td>
<td>a string or dictionary, depending on the tool</td>
<td>Depends on the tool</td>
</tr>
</tbody>
</table>
</div>
<p>Runable对组件的输入进行了强制统一（注意：若执行有报错，很大的原因则源头报错）</p>
<p>第二个参数：RunableConfig是在运行时态的一种定义，比如回调等</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Attribute</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>run_name</td>
<td>Name used for the given Runnable(not iherited)</td>
</tr>
<tr>
<td>run_id</td>
<td>Unique identifier  for this call. sub-calls will get their own unique run ids</td>
</tr>
<tr>
<td>tags</td>
<td>Tags for this call and sub-calls</td>
</tr>
<tr>
<td>metadata</td>
<td>Metadata for this call and any sub-calls</td>
</tr>
<tr>
<td>callbacks</td>
<td>Callbacks for this class and any sub-calls</td>
</tr>
<tr>
<td>max_concurrency</td>
<td>Maximum number of parallel calls to make(e.g. used by batch)</td>
</tr>
<tr>
<td>recursion_limit</td>
<td>Maximum number of times a call can recurse(e.g. used by Runnables that return Runnables)</td>
</tr>
<tr>
<td>configurable</td>
<td>Runtime values for configurable attributes of the Runnable</td>
</tr>
</tbody>
</table>
</div>
<h2 id="为什么使用-Runnable"><a href="#为什么使用-Runnable" class="headerlink" title="为什么使用 Runnable"></a>为什么使用 <code>Runnable</code></h2><div class="table-container">
<table>
<thead>
<tr>
<th>特性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>标准化接口</strong></td>
<td>所有组件都可以 <code>.invoke()</code>/ <code>.stream()</code>/ <code>.batch()</code></td>
</tr>
<tr>
<td><strong>可组合性强</strong></td>
<td>可通过 \</td>
<td>进行链式调用</td>
</tr>
<tr>
<td><strong>可观察性强</strong></td>
<td>所有执行都可以加 callback，用于调试、追踪、评估</td>
</tr>
<tr>
<td><strong>支持异步</strong></td>
<td>支持同步与异步执行 <code>.ainvoke()</code>，<code>.astream()</code></td>
</tr>
</tbody>
</table>
</div>
<h2 id="Runnable-的主要方法"><a href="#Runnable-的主要方法" class="headerlink" title="Runnable 的主要方法"></a><code>Runnable</code> 的主要方法</h2><div class="table-container">
<table>
<thead>
<tr>
<th>方法名</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>.invoke(input)</code></td>
<td>同步执行</td>
<td>接收一个输入，返回一个输出</td>
</tr>
<tr>
<td><code>.ainvoke(input)</code></td>
<td>异步执行</td>
<td>异步方式执行</td>
</tr>
<tr>
<td><code>.stream(input)</code></td>
<td>流式响应</td>
<td>用于返回响应片段（如聊天消息 chunk）</td>
</tr>
<tr>
<td><code>.batch(inputs)</code></td>
<td>批处理</td>
<td>接收多个输入列表，返回多个输出</td>
</tr>
<tr>
<td><code>.bind(config)</code></td>
<td>配置绑定</td>
<td>绑定静态配置，如 <code>stop</code>, <code>temperature</code>等</td>
</tr>
</tbody>
</table>
</div>
<h2 id="哪些组件是-Runnable"><a href="#哪些组件是-Runnable" class="headerlink" title="哪些组件是 Runnable"></a>哪些组件是 <code>Runnable</code></h2><div class="table-container">
<table>
<thead>
<tr>
<th>组件类型</th>
<th>实现 <code>Runnable</code> 接口？</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ChatOpenAI</code>/ <code>ChatAnthropic</code></td>
<td>是</td>
</tr>
<tr>
<td><code>PromptTemplate</code> / <code>ChatPromptTemplate</code></td>
<td>是</td>
</tr>
<tr>
<td><code>StrOutputParser</code> / <code>JsonOutputParser</code></td>
<td>是</td>
</tr>
<tr>
<td><code>RunnableLambda</code>/ <code>RunnableMap</code></td>
<td>是</td>
</tr>
<tr>
<td>自定义函数包装器 <code>RunnableLambda(fn)</code></td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<h1 id="LCEL简介"><a href="#LCEL简介" class="headerlink" title="LCEL简介"></a>LCEL简介</h1><p>LangChain LCEL的全称为LangChain Expression Language即可直译为LangChain表达式。</p>
<p>为了构造更复杂的LLM应用并且更为简便快捷的构造LLM应用，LangChain提供了类似”管道“的形式去声明提示词模版（prompt），即用”|“来连接各个组件之间的操作。也就是LCEL允许开发者将不同的模块进行简单的形式视线串联。语法如下所示：</p>
<p><code>chain = 提示词模板 | 大模型调用 | 输出解析器</code></p>
<p><img src="https://jing-images.oss-cn-beijing.aliyuncs.com/img/image-20250904094128108.png" alt="image-20250904094128108" style="zoom:50%;" /></p>
<h1 id="LCEL的好处"><a href="#LCEL的好处" class="headerlink" title="LCEL的好处"></a>LCEL的好处</h1><ul>
<li>优化的并行执行</li>
<li>稳定的异步支持</li>
<li>简化流式传输</li>
<li>无缝植入LangSmith追踪</li>
<li>标准API</li>
<li>可使用LangServe部署（FastAPI + LCEL）</li>
</ul>
<h1 id="LCEL封装核心内容"><a href="#LCEL封装核心内容" class="headerlink" title="LCEL封装核心内容"></a>LCEL封装核心内容</h1><p>Runnables的API方法：<a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/core/runnables.html">https://python.langchain.com/api_reference/core/runnables.html</a></p>
<p><img src="https://jing-images.oss-cn-beijing.aliyuncs.com/img/image-20250904094645701.png" alt="image-20250904094645701" style="zoom:67%;" /></p>
<p>在使用LCEL表达式时，需要先了解其中包含的元素：</p>
<ul>
<li><code>|</code>：连接符</li>
<li><code>Runnable对象</code>：可执行操作</li>
</ul>
<h1 id="LCEL使用场景"><a href="#LCEL使用场景" class="headerlink" title="LCEL使用场景"></a>LCEL使用场景</h1><p>使用场景</p>
<ul>
<li>单个大模型调用：无需LCEL，直接调用模型即可（相当于简单使用deepseek）</li>
<li>简单链（llm + 提示词 + 解析器），使用LCEL最佳场景</li>
<li>构建复杂链（分支、循环、多个智能体等），使用LangGraph</li>
<li>LCEL可以嵌入在绝大部分LangChain生态应用里</li>
</ul>
<p>旧版本中的预制链</p>
<ul>
<li>V0.0版本中存在大量的预制链（LLMChain、CoversationChain…）</li>
<li>在0.3版本中依然支持但建议弃用（提示词不可见无法自定义等）</li>
<li>建议使用LCEL或LangGraph方式进行自定义重构</li>
<li>迁移示例：LLMChain StuffDocumentsChain</li>
</ul>
<h1 id="链的基本使用"><a href="#链的基本使用" class="headerlink" title="链的基本使用"></a>链的基本使用</h1><ul>
<li>使用管道操作符快速生成一条链</li>
<li>链的流式调用</li>
<li>并行运行多条链</li>
<li>链的调试</li>
</ul>
<h2 id="使用管道操作符快速生成一条链"><a href="#使用管道操作符快速生成一条链" class="headerlink" title="使用管道操作符快速生成一条链"></a>使用管道操作符快速生成一条链</h2><p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用管道操作符来实现一条链</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;讲一个关于&#123;topic&#125;的笑话,不要有任何解释&quot;</span>)</span><br><span class="line"></span><br><span class="line">chain = prompt | llm | StrOutputParser()</span><br><span class="line">res=chain.invoke(&#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;鲜花&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<h2 id="链的pipe方式"><a href="#链的pipe方式" class="headerlink" title="链的pipe方式"></a>链的pipe方式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;讲一个关于&#123;topic&#125;的笑话,不要有任何解释&quot;</span>)</span><br><span class="line"></span><br><span class="line">chain = (prompt.pipe(llm).pipe(StrOutputParser()))</span><br><span class="line">res=chain.invoke(&#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;狗&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<h2 id="链的流式调用"><a href="#链的流式调用" class="headerlink" title="链的流式调用"></a>链的流式调用</h2><ul>
<li>异步调用astream</li>
<li>JSON流输出</li>
<li>使用流事件</li>
</ul>
<p>注意：不是所有的组件都支持流式输出，当不支持的组件呗封装在chain中后，最后的结果依旧可以使用流输出</p>
<h3 id="异步调用astream"><a href="#异步调用astream" class="headerlink" title="异步调用astream"></a>异步调用astream</h3><ul>
<li>stream: 同步调用</li>
<li>astream: 异步调用</li>
</ul>
<p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;讲一个关于&#123;topic&#125;的笑话,不要有任何解释&quot;</span>)</span><br><span class="line"></span><br><span class="line">chain = prompt | llm | StrOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># for chunk in chain.stream(&quot;小狗&quot;):</span></span><br><span class="line"><span class="comment">#     print(chunk, end=&quot;|&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 异步调用</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">for</span> chunk <span class="keyword">in</span> chain.astream(<span class="string">&quot;小狗&quot;</span>):</span><br><span class="line">    <span class="built_in">print</span>(chunk, end=<span class="string">&quot;|&quot;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="JSON流输出"><a href="#JSON流输出" class="headerlink" title="JSON流输出"></a>JSON流输出</h3><p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> JsonOutputParser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个链，将LLM的输出通过JsonOutputParser进行解析</span></span><br><span class="line"><span class="comment"># 注意：由于旧版本Langchain的一个bug，JsonOutputParser可能无法正确流式处理某些模型的结果</span></span><br><span class="line">chain = ( llm | JsonOutputParser())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用astream方法进行异步流式处理</span></span><br><span class="line"><span class="comment"># 发送提示要求模型以JSON格式输出法国、西班牙和日本的国家及其人口信息</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">for</span> text <span class="keyword">in</span> chain.astream(</span><br><span class="line">    <span class="string">&quot;以JSON格式输出法国、西班牙和日本的国家及其人口信息&quot;</span></span><br><span class="line">    <span class="string">&quot;使用一个字典，包含外层键名为&#x27;countries&#x27;的国家列表&quot;</span></span><br><span class="line">    <span class="string">&quot;每个国家应该有键名为&#x27;name&#x27;和&#x27;population&#x27;&quot;</span></span><br><span class="line">):</span><br><span class="line">    <span class="comment"># 打印每个流式返回的文本片段，并立即刷新输出缓冲区</span></span><br><span class="line">    <span class="built_in">print</span>(text, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="事件流"><a href="#事件流" class="headerlink" title="事件流"></a>事件流</h3><ul>
<li><p>这是一个测试事件</p>
</li>
<li><p>可以将流的过程进行分解，从而事件更细颗粒度的控制</p>
</li>
<li><p>langchain-core &gt;= 0.2</p>
</li>
<li><p>事件流的颗粒度：</p>
<p>| event                | name             | chunk                           | input                                        | output                                         |<br>| —————————— | ———————— | ———————————————- | —————————————————————— | ——————————————————————— |<br>| on_chat_model_start  | [model name]     |                                 | {“messages”:[[SystemMessage, HumanMessage]]} |                                                |<br>| on_chat_model_stream | [model name]     | AIMessageChunk(content=”hello”) |                                              |                                                |<br>| on_chat_model_end1   | [model name]     |                                 | {“messages”:[[SystemMessage, HumanMessage]]} | AIMessageChunk(content=”hello world”)          |<br>| on_llm_start         | [model name]     |                                 | {“input”: “hello”}                           |                                                |<br>| on_llm_stream        | [model name]     | ‘Hello’                         |                                              |                                                |<br>| on_llm_end           | [model name]     |                                 | ‘Hello human!’                               |                                                |<br>| on_chain_start       | format_docs      |                                 |                                              |                                                |<br>| on_chain_stream      | format_docs      | “hello world!, goodbye world!”  |                                              |                                                |<br>| on_chain_end         | format_docs      |                                 | [Document(…)]                              | “hello world!, goodbye”                        |<br>| on_tool_start15      | some_tool16      |                                 | {“x”: 1, “y”: “2”}                           |                                                |<br>| on_tool_end18        | some_tool19      |                                 |                                              | {“x”: 1, “y”: “2”}                             |<br>| on_retriever_start   | [retriever name] |                                 | {“query”: “hello”}                           |                                                |<br>| on_retriever_end     | [retriever name] |                                 | {“query”: “hello”}                           | [Document(…), …]                           |<br>| on_prompt_start      | [template_name]  |                                 | {“question”: “hello”}                        |                                                |<br>| on_prompt_end        | [template_name]  |                                 | {“question”: “hello”}                        | ChatPromptValue(messages=[SystemMessage, …]) |</p>
<p>注意对于版本langchain-core&lt;0.3.37，需要显式地指定事件流版本</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">events = []</span><br><span class="line"><span class="keyword">async</span> <span class="keyword">for</span> event <span class="keyword">in</span> llm.astream_events(<span class="string">&quot;hello&quot;</span>,version=<span class="string">&quot;v2&quot;</span>):</span><br><span class="line">    events.append(event)</span><br></pre></td></tr></table></figure>
<h4 id="事件过滤-按name"><a href="#事件过滤-按name" class="headerlink" title="事件过滤-按name"></a>事件过滤-按name</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> JsonOutputParser</span><br><span class="line"></span><br><span class="line">chain = llm.with_config(&#123;<span class="string">&quot;run_name&quot;</span>: <span class="string">&quot;model&quot;</span>&#125;) | JsonOutputParser().with_config(</span><br><span class="line">    &#123;<span class="string">&quot;run_name&quot;</span>: <span class="string">&quot;my_parser&quot;</span>&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">max_events = <span class="number">0</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">for</span> event <span class="keyword">in</span> chain.astream_events(</span><br><span class="line">    <span class="string">&quot;output a list of the countries france, spain and japan and their populations in JSON format. &quot;</span></span><br><span class="line">    <span class="string">&#x27;Use a dict with an outer key of &quot;countries&quot; which contains a list of countries. &#x27;</span></span><br><span class="line">    <span class="string">&quot;Each country should have the key `name` and `population`&quot;</span>,</span><br><span class="line">    include_names=[<span class="string">&quot;my_parser&quot;</span>],version=<span class="string">&quot;v2&quot;</span></span><br><span class="line">):</span><br><span class="line">    <span class="built_in">print</span>(event)</span><br><span class="line">    max_events += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> max_events &gt; <span class="number">10</span>:</span><br><span class="line">        <span class="comment"># Truncate output</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;...&quot;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="事件过滤-按tag"><a href="#事件过滤-按tag" class="headerlink" title="事件过滤 - 按tag"></a>事件过滤 - 按tag</h4><p>按照tag来过滤事件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> JsonOutputParser</span><br><span class="line"></span><br><span class="line">chain = (llm | JsonOutputParser()).with_config(&#123;<span class="string">&quot;tags&quot;</span>: [<span class="string">&quot;my_chain&quot;</span>]&#125;)</span><br><span class="line"></span><br><span class="line">max_events = <span class="number">0</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">for</span> event <span class="keyword">in</span> chain.astream_events(</span><br><span class="line">    <span class="string">&#x27;output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of &quot;countries&quot; which contains a list of countries. Each country should have the key `name` and `population`&#x27;</span>,</span><br><span class="line">    include_tags=[<span class="string">&quot;my_chain&quot;</span>],version=<span class="string">&quot;v2&quot;</span></span><br><span class="line">):</span><br><span class="line">    <span class="built_in">print</span>(event)</span><br><span class="line">    max_events += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> max_events &gt; <span class="number">10</span>:</span><br><span class="line">        <span class="comment"># Truncate output</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;...&quot;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h4 id="事件阶段过滤"><a href="#事件阶段过滤" class="headerlink" title="事件阶段过滤"></a>事件阶段过滤</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> JsonOutputParser</span><br><span class="line"></span><br><span class="line">chain = (llm | JsonOutputParser()).with_config(&#123;<span class="string">&quot;tags&quot;</span>: [<span class="string">&quot;my_chain&quot;</span>]&#125;)</span><br><span class="line"></span><br><span class="line">num_events = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">for</span> event <span class="keyword">in</span> chain.astream_events(</span><br><span class="line">    <span class="string">&quot;output a list of the countries france, spain and japan and their populations in JSON format. &quot;</span></span><br><span class="line">    <span class="string">&#x27;Use a dict with an outer key of &quot;countries&quot; which contains a list of countries. &#x27;</span></span><br><span class="line">    <span class="string">&quot;Each country should have the key `name` and `population`&quot;</span>,version=<span class="string">&quot;v2&quot;</span></span><br><span class="line">):</span><br><span class="line">    kind = event[<span class="string">&quot;event&quot;</span>]</span><br><span class="line">    <span class="keyword">if</span> kind == <span class="string">&quot;on_chat_model_stream&quot;</span>:</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">f&quot;Chat model chunk: <span class="subst">&#123;<span class="built_in">repr</span>(event[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;chunk&#x27;</span>].content)&#125;</span>&quot;</span>,</span><br><span class="line">            flush=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">if</span> kind == <span class="string">&quot;on_parser_stream&quot;</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Parser chunk: <span class="subst">&#123;event[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;chunk&#x27;</span>]&#125;</span>&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    num_events += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> num_events &gt; <span class="number">30</span>:</span><br><span class="line">        <span class="comment"># Truncate the output</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;...&quot;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h2 id="并行运行多条链"><a href="#并行运行多条链" class="headerlink" title="并行运行多条链"></a>并行运行多条链</h2><p>并行运行链会构造：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    Input</span><br><span class="line">     / \</span><br><span class="line">    /   \</span><br><span class="line">Branch1 Branch2</span><br><span class="line">    \   /</span><br><span class="line">     \ /</span><br><span class="line">     Combine</span><br></pre></td></tr></table></figure>
<p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableParallel</span><br><span class="line"></span><br><span class="line">joke_chain = ChatPromptTemplate.from_template(<span class="string">&quot;给我讲一个关于&#123;topic&#125;的笑话&quot;</span>) | llm</span><br><span class="line">poem_chain = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;给我写一首关于&#123;topic&#125;的绝句&quot;</span>) | llm</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)</span><br><span class="line"></span><br><span class="line">map_chain.invoke(&#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;程序员&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>查看图，需要安装依赖<code>pip install grandalf</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">map_chain.get_graph().print_ascii()</span><br></pre></td></tr></table></figure>
<p>执行结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">               +--------------------------+                </span><br><span class="line">               | Parallel&lt;joke,poem&gt;Input |                </span><br><span class="line">               +--------------------------+                </span><br><span class="line">                   ***               ***                   </span><br><span class="line">                ***                     ***                </span><br><span class="line">              **                           **              </span><br><span class="line">+--------------------+              +--------------------+ </span><br><span class="line">| ChatPromptTemplate |              | ChatPromptTemplate | </span><br><span class="line">+--------------------+              +--------------------+ </span><br><span class="line">           *                                   *           </span><br><span class="line">           *                                   *           </span><br><span class="line">           *                                   *           </span><br><span class="line">    +------------+                      +------------+     </span><br><span class="line">    | ChatOpenAI |                      | ChatOpenAI |     </span><br><span class="line">    +------------+*                     +------------+     </span><br><span class="line">                   ***               ***                   </span><br><span class="line">                      ***         ***                      </span><br><span class="line">                         **     **                         </span><br><span class="line">              +---------------------------+                </span><br><span class="line">              | Parallel&lt;joke,poem&gt;Output |                </span><br><span class="line">              +---------------------------+    </span><br></pre></td></tr></table></figure>
<p>查看提示词</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">map_chain.get_prompts()</span><br></pre></td></tr></table></figure>
<p>执行结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ChatPromptTemplate(input_variables=[&#x27;topic&#x27;], input_types=&#123;&#125;, partial_variables=&#123;&#125;, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;topic&#x27;], input_types=&#123;&#125;, partial_variables=&#123;&#125;, template=&#x27;给我讲一个关于&#123;topic&#125;的笑话&#x27;), additional_kwargs=&#123;&#125;)]),</span><br><span class="line"> ChatPromptTemplate(input_variables=[&#x27;topic&#x27;], input_types=&#123;&#125;, partial_variables=&#123;&#125;, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;topic&#x27;], input_types=&#123;&#125;, partial_variables=&#123;&#125;, template=&#x27;给我写一首关于&#123;topic&#125;的绝句&#x27;), additional_kwargs=&#123;&#125;)])]</span><br></pre></td></tr></table></figure>
<h1 id="老版本的chain迁移到LCEL"><a href="#老版本的chain迁移到LCEL" class="headerlink" title="老版本的chain迁移到LCEL"></a>老版本的chain迁移到LCEL</h1><h2 id="示例1-从LLMChain迁移"><a href="#示例1-从LLMChain迁移" class="headerlink" title="示例1:从LLMChain迁移"></a>示例1:从LLMChain迁移</h2><h3 id="废弃的用法"><a href="#废弃的用法" class="headerlink" title="废弃的用法"></a>废弃的用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> LLMChain</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">     [</span><br><span class="line">         (<span class="string">&quot;user&quot;</span>, <span class="string">&quot;Tell me a &#123;adjective&#125; joke&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">legacy_chain = LLMChain(llm=llm, prompt=prompt)</span><br><span class="line">legacy_chain.run(&#123;<span class="string">&quot;adjective&quot;</span>: <span class="string">&quot;funny&quot;</span>&#125;)</span><br><span class="line">legacy_chain</span><br></pre></td></tr></table></figure>
<h3 id="LCEL的用法"><a href="#LCEL的用法" class="headerlink" title="LCEL的用法"></a>LCEL的用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">     [</span><br><span class="line">         (<span class="string">&quot;user&quot;</span>, <span class="string">&quot;Tell me a &#123;adjective&#125; joke&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">chain = prompt | llm | StrOutputParser()</span><br><span class="line">chain.invoke(&#123;<span class="string">&quot;adjective&quot;</span>: <span class="string">&quot;funny&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="示例2-从StuffDocumentsChain迁移"><a href="#示例2-从StuffDocumentsChain迁移" class="headerlink" title="示例2: 从StuffDocumentsChain迁移"></a>示例2: 从StuffDocumentsChain迁移</h2><p>是一个用于文档内容总结的预制链</p>
<h3 id="废弃的用法-1"><a href="#废弃的用法-1" class="headerlink" title="废弃的用法"></a>废弃的用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document</span><br><span class="line"></span><br><span class="line">documents = [</span><br><span class="line">    Document(page_content=<span class="string">&quot;Apples are red&quot;</span>, metadata=&#123;<span class="string">&quot;title&quot;</span>: <span class="string">&quot;apple_book&quot;</span>&#125;),</span><br><span class="line">    Document(page_content=<span class="string">&quot;Blueberries are blue&quot;</span>, metadata=&#123;<span class="string">&quot;title&quot;</span>: <span class="string">&quot;blueberry_book&quot;</span>&#125;),</span><br><span class="line">    Document(page_content=<span class="string">&quot;Bananas are yelow&quot;</span>, metadata=&#123;<span class="string">&quot;title&quot;</span>: <span class="string">&quot;banana_book&quot;</span>&#125;),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> LLMChain, StuffDocumentsChain</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># This controls how each document will be formatted. Specifically,</span></span><br><span class="line"><span class="comment"># it will be passed to `format_document` - see that function for more</span></span><br><span class="line"><span class="comment"># details.</span></span><br><span class="line">document_prompt = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;page_content&quot;</span>], template=<span class="string">&quot;&#123;page_content&#125;&quot;</span></span><br><span class="line">)</span><br><span class="line">document_variable_name = <span class="string">&quot;context&quot;</span></span><br><span class="line"><span class="comment"># The prompt here should take as an input variable the</span></span><br><span class="line"><span class="comment"># `document_variable_name`</span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;Summarize this content: &#123;context&#125;&quot;</span>)</span><br><span class="line">llm_chain = LLMChain(llm=llm, prompt=prompt)</span><br><span class="line">chain = StuffDocumentsChain(</span><br><span class="line">    llm_chain=llm_chain,</span><br><span class="line">    document_prompt=document_prompt,</span><br><span class="line">    document_variable_name=document_variable_name,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">result = chain.invoke(documents)</span><br><span class="line">result[<span class="string">&quot;output_text&quot;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="LCEL用法"><a href="#LCEL用法" class="headerlink" title="LCEL用法"></a>LCEL用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">from</span> langchain.chains.combine_documents <span class="keyword">import</span> create_stuff_documents_chain</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line">documents = [</span><br><span class="line">    Document(page_content=<span class="string">&quot;Apples are red&quot;</span>, metadata=&#123;<span class="string">&quot;title&quot;</span>: <span class="string">&quot;apple_book&quot;</span>&#125;),</span><br><span class="line">    Document(page_content=<span class="string">&quot;Blueberries are blue&quot;</span>, metadata=&#123;<span class="string">&quot;title&quot;</span>: <span class="string">&quot;blueberry_book&quot;</span>&#125;),</span><br><span class="line">    Document(page_content=<span class="string">&quot;Bananas are yelow&quot;</span>, metadata=&#123;<span class="string">&quot;title&quot;</span>: <span class="string">&quot;banana_book&quot;</span>&#125;),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;Summarize this content: &#123;context&#125;&quot;</span>)</span><br><span class="line">chain = create_stuff_documents_chain(llm, prompt)</span><br><span class="line"></span><br><span class="line">result = chain.invoke(&#123;<span class="string">&quot;context&quot;</span>: documents&#125;)</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<h1 id="链的高级应用"><a href="#链的高级应用" class="headerlink" title="链的高级应用"></a>链的高级应用</h1><p>链（Chain）的高级使用技巧，包括如何将函数通过修饰符快速转换为链、在链中使用Lambda函数、自定义支持流式输出的函数、链中的传值方式（如config和记忆功能）、以及创建路由链和回退机制等。重点介绍了函数在链中的灵活应用，使开发者可以更方便地构建复杂的应用逻辑。</p>
<h2 id="在链中使用函数"><a href="#在链中使用函数" class="headerlink" title="在链中使用函数"></a>在链中使用函数</h2><h3 id="使用-chain快速将函数转为链"><a href="#使用-chain快速将函数转为链" class="headerlink" title="使用@chain快速将函数转为链"></a>使用@chain快速将函数转为链</h3><p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入字符串输出解析器</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="comment"># 导入chain装饰器，用于创建自定义链</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> chain</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建第一个提示模板：请求关于特定主题的笑话</span></span><br><span class="line">prompt1 = ChatPromptTemplate.from_template(<span class="string">&quot;Tell me a joke about &#123;topic&#125;&quot;</span>)</span><br><span class="line"><span class="comment"># 创建第二个提示模板：询问笑话的主题是什么</span></span><br><span class="line">prompt2 = ChatPromptTemplate.from_template(<span class="string">&quot;What is the subject of this joke: &#123;joke&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用@chain装饰器定义一个自定义链</span></span><br><span class="line"><span class="meta">@chain</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">custom_chain</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># 步骤1: 将输入文本填充到第一个提示模板中</span></span><br><span class="line">    prompt_val1 = prompt1.invoke(&#123;<span class="string">&quot;topic&quot;</span>: text&#125;)</span><br><span class="line">    <span class="comment"># 步骤2: 使用大模型生成关于指定主题的笑话</span></span><br><span class="line">    output1 = llm.invoke(prompt_val1)</span><br><span class="line">    <span class="comment"># 步骤3: 将模型输出解析为字符串</span></span><br><span class="line">    parsed_output1 = StrOutputParser().invoke(output1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 步骤4: 创建第二个处理链，用于分析笑话主题</span></span><br><span class="line">    <span class="comment"># 这个链将提示模板、DS模型和字符串解析器串联起来</span></span><br><span class="line">    chain2 = prompt2 | llm | StrOutputParser()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 步骤5: 将第一步生成的笑话作为输入，让第二个链分析其主题</span></span><br><span class="line">    <span class="keyword">return</span> chain2.invoke(&#123;<span class="string">&quot;joke&quot;</span>: parsed_output1&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用自定义链，输入主题&quot;bears&quot;（熊）</span></span><br><span class="line"><span class="comment"># 整个过程：</span></span><br><span class="line"><span class="comment"># 1. 先生成一个关于熊的笑话</span></span><br><span class="line"><span class="comment"># 2. 然后分析这个笑话的主题是什么</span></span><br><span class="line"><span class="comment"># 3. 返回分析结果</span></span><br><span class="line">custom_chain.invoke(<span class="string">&quot;bears&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="链中使用Lambda"><a href="#链中使用Lambda" class="headerlink" title="链中使用Lambda"></a>链中使用Lambda</h3><p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter  <span class="comment"># 导入itemgetter函数，用于从字典中提取值</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate  <span class="comment"># 导入聊天提示模板</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableLambda  <span class="comment"># 导入可运行的Lambda函数包装器</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">length_function</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_multiple_length_function</span>(<span class="params">text1, text2</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(text1) + <span class="built_in">len</span>(text2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiple_length_function</span>(<span class="params">_<span class="built_in">dict</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _multiple_length_function(_<span class="built_in">dict</span>[<span class="string">&quot;text1&quot;</span>], _<span class="built_in">dict</span>[<span class="string">&quot;text2&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个简单的聊天提示模板，询问a和b的和</span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;what is &#123;a&#125; + &#123;b&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个复杂的处理链</span></span><br><span class="line">chain = (</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># 处理&quot;a&quot;参数:</span></span><br><span class="line">        <span class="comment"># 1. 从输入字典中提取&quot;foo&quot;键的值</span></span><br><span class="line">        <span class="comment"># 2. 将提取的值传递给length_function函数(假设这个函数计算字符串长度)</span></span><br><span class="line">        <span class="comment"># RunnableLambda 运行的是Lambda函数</span></span><br><span class="line">        <span class="string">&quot;a&quot;</span>: itemgetter(<span class="string">&quot;foo&quot;</span>) | RunnableLambda(length_function),</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 处理&quot;b&quot;参数:</span></span><br><span class="line">        <span class="comment"># 1. 创建一个包含两个键值对的字典:</span></span><br><span class="line">        <span class="comment">#    - &quot;text1&quot;: 从输入字典中提取&quot;foo&quot;键的值</span></span><br><span class="line">        <span class="comment">#    - &quot;text2&quot;: 从输入字典中提取&quot;bar&quot;键的值</span></span><br><span class="line">        <span class="comment"># 2. 将这个字典传递给multiple_length_function函数</span></span><br><span class="line">        <span class="comment">#    (假设这个函数计算两个文本的总长度)</span></span><br><span class="line">        <span class="string">&quot;b&quot;</span>: &#123;<span class="string">&quot;text1&quot;</span>: itemgetter(<span class="string">&quot;foo&quot;</span>), <span class="string">&quot;text2&quot;</span>: itemgetter(<span class="string">&quot;bar&quot;</span>)&#125;</span><br><span class="line">        | RunnableLambda(multiple_length_function),</span><br><span class="line">    &#125;</span><br><span class="line">    | prompt  <span class="comment"># 将处理后的&quot;a&quot;和&quot;b&quot;值填入提示模板</span></span><br><span class="line">    | llm  <span class="comment"># 将填充后的提示发送给大模型生成回答</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用链处理流程，输入一个包含&quot;foo&quot;和&quot;bar&quot;键的字典</span></span><br><span class="line"><span class="comment"># 整个过程:</span></span><br><span class="line"><span class="comment"># 1. 计算&quot;bar&quot;字符串的长度作为a的值</span></span><br><span class="line"><span class="comment"># 2. 计算&quot;bar&quot;和&quot;gah&quot;字符串的总长度作为b的值</span></span><br><span class="line"><span class="comment"># 3. 将这些值填入提示&quot;what is &#123;a&#125; + &#123;b&#125;&quot;</span></span><br><span class="line"><span class="comment"># 4. 让DeepSeek模型回答这个问题</span></span><br><span class="line">chain.invoke(&#123;<span class="string">&quot;foo&quot;</span>: <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;bar&quot;</span>: <span class="string">&quot;gah&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="在链中自定义支持流输出的函数"><a href="#在链中自定义支持流输出的函数" class="headerlink" title="在链中自定义支持流输出的函数"></a>在链中自定义支持流输出的函数</h2><ul>
<li>当链被使用stream或astream调用的时候</li>
<li>如何在链中增加自定义函数</li>
</ul>
<h3 id="一个简单的链支持流调用"><a href="#一个简单的链支持流调用" class="headerlink" title="一个简单的链支持流调用"></a>一个简单的链支持流调用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个聊天提示模板，要求生成5个与给定动物相似的动物名称，以都好分隔</span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;请列出5个与以下动物相似的动物名称，用逗号分隔： &#123;animal&#125;, 不要包含数字。&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个处理链：提示词模板 -》 模型 -》 字符串输出解析器</span></span><br><span class="line">str_chain = prompt | llm | StrOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 流式输出，输入为“熊”</span></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> str_chain.stream(prompt.invoke(&#123;<span class="string">&quot;animal&quot;</span>: <span class="string">&quot;熊&quot;</span>&#125;)):</span><br><span class="line">    <span class="built_in">print</span>(chunk, end=<span class="string">&quot;&quot;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="增加自定义函数"><a href="#增加自定义函数" class="headerlink" title="增加自定义函数"></a>增加自定义函数</h3><ul>
<li>聚合当前流的输出</li>
<li>在生成下一个逗号的时候组合</li>
<li>注意：使用了yield</li>
</ul>
<p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个聊天提示模板，要求生成5个与给定动物相似的动物名称，以都好分隔</span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;请列出5个与以下动物相似的动物名称，用逗号分隔： &#123;animal&#125;, 不要包含数字。&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个处理链：提示词模板 -》 模型 -》 字符串输出解析器</span></span><br><span class="line">str_chain = prompt | llm | StrOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Iterator, <span class="type">List</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是一个自定义解析器，将LLM输出的标记迭代器</span></span><br><span class="line"><span class="comment"># 按逗号分隔转换为字符串列表</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_into_list</span>(<span class="params"><span class="built_in">input</span>: Iterator[<span class="built_in">str</span>]</span>) -&gt; Iterator[<span class="type">List</span>[<span class="built_in">str</span>]]:</span><br><span class="line">    <span class="comment"># 保存部分输入直到遇到逗号</span></span><br><span class="line">    buffer = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> chunk <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">        <span class="comment"># 将当前块添加到缓冲区</span></span><br><span class="line">        buffer += chunk</span><br><span class="line">        <span class="comment"># 当缓冲区中有逗号时</span></span><br><span class="line">        <span class="keyword">while</span> <span class="string">&quot;,&quot;</span> <span class="keyword">in</span> buffer:</span><br><span class="line">            <span class="comment"># 在逗号处分割缓冲区</span></span><br><span class="line">            comma_index = buffer.index(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            <span class="comment"># 输出逗号之前的所有内容</span></span><br><span class="line">            <span class="keyword">yield</span> [buffer[:comma_index].strip()]</span><br><span class="line">            <span class="comment"># 保存剩余部分用于下一次迭代</span></span><br><span class="line">            buffer = buffer[comma_index + <span class="number">1</span> :]</span><br><span class="line">    <span class="comment"># 输出最后一块</span></span><br><span class="line">    <span class="keyword">yield</span> [buffer.strip()]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">list_chain = str_chain | split_into_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> list_chain.stream(&#123;<span class="string">&quot;animal&quot;</span>: <span class="string">&quot;熊&quot;</span>&#125;):</span><br><span class="line">    <span class="built_in">print</span>(chunk, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="扩展：yeild与return区别"><a href="#扩展：yeild与return区别" class="headerlink" title="扩展：yeild与return区别"></a>扩展：yeild与return区别</h4><ul>
<li>return 函数立即计算并返回所有结果，而yield函数按需计算结果</li>
<li>return 函数返回一个数据结构（如列表），yield函数返回一个生成器对象</li>
<li>yield函数可以处理潜在的无限序列，而return函数必须在有限时间内完成</li>
<li>生成器对象是一次性的，遍历完后就被消耗完毕，而return返回的数据结构可以重复使用</li>
<li>yield特别适合处理大数据集或流式数据，因为它不需要一次性将所有数据加载到内存中</li>
</ul>
<p>return使用代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用return</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_squares_return</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回包含0到n-1的平方数的列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        result.append(i * i)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用return函数</span></span><br><span class="line">squares = get_squares_return(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 一次性获取所有结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;使用return的结果：&quot;</span>, squares)</span><br><span class="line"><span class="comment"># 返回类型是列表</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;类型：&quot;</span>, <span class="built_in">type</span>(squares))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历结果</span></span><br><span class="line"><span class="keyword">for</span> square <span class="keyword">in</span> squares:</span><br><span class="line">    <span class="built_in">print</span>(square)</span><br></pre></td></tr></table></figure>
<p>yeild使用代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用yield</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_squares_yield</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回包含0到n-1的平方数的列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">yield</span> i * i <span class="comment"># 每次生成一个结果并暂停</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用yield函数</span></span><br><span class="line">squares = get_squares_yield(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 返回一个生成器对象</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;使用yield的结果：&quot;</span>, squares)</span><br><span class="line"><span class="comment"># 返回类型是生成器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;类型：&quot;</span>, <span class="built_in">type</span>(squares))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历生成器</span></span><br><span class="line"><span class="keyword">for</span> square <span class="keyword">in</span> squares:</span><br><span class="line">    <span class="built_in">print</span>(square) <span class="comment"># 每次迭代时才计算下一个值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次遍历生成器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;再次遍历：&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> square <span class="keyword">in</span> squares:</span><br><span class="line">    <span class="built_in">print</span>(square) <span class="comment"># 不会输出任何内容，因为生成器已经耗尽完毕</span></span><br></pre></td></tr></table></figure>
<h2 id="使用RunnablePassthrough来传递值"><a href="#使用RunnablePassthrough来传递值" class="headerlink" title="使用RunnablePassthrough来传递值"></a>使用RunnablePassthrough来传递值</h2><p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableParallel, RunnablePassthrough</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个可并行运行的处理流程</span></span><br><span class="line">runnable = RunnableParallel(</span><br><span class="line">    passed=RunnablePassthrough(),  <span class="comment"># 第一个处理器：直接传递输入，不做修改</span></span><br><span class="line">    modified=<span class="keyword">lambda</span> x: x[<span class="string">&quot;num&quot;</span>] + <span class="number">1</span>,  <span class="comment"># 第二个处理器：取出输入中的&quot;num&quot;值并加1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行这个处理流程，输入是一个包含&quot;num&quot;字段的字典</span></span><br><span class="line">runnable.invoke(&#123;<span class="string">&quot;num&quot;</span>: <span class="number">1</span>&#125;)</span><br><span class="line"><span class="comment"># 运行结果：&#123;&#x27;passed&#x27;: &#123;&#x27;num&#x27;: 1&#125;, &#x27;modified&#x27;: 2&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="如何在运行时动态添加链的配置"><a href="#如何在运行时动态添加链的配置" class="headerlink" title="如何在运行时动态添加链的配置"></a>如何在运行时动态添加链的配置</h2><h3 id="动态改写模型温度"><a href="#动态改写模型温度" class="headerlink" title="动态改写模型温度"></a>动态改写模型温度</h3><p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> ConfigurableField</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>),</span><br><span class="line">    api_key= os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line">).configurable_fields(</span><br><span class="line">    temperature=ConfigurableField(</span><br><span class="line">        <span class="built_in">id</span>=<span class="string">&quot;llm_temperature&quot;</span>,</span><br><span class="line">        name=<span class="string">&quot;LLM Temperature&quot;</span>,</span><br><span class="line">        description=<span class="string">&quot;Temperature for the model&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">llm.invoke(<span class="string">&quot;随意挑选一个随机数，输出为一个整数&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在运行时改写温度</span></span><br><span class="line">llm.with_config(configurable=&#123;<span class="string">&quot;llm_temperature&quot;</span>: <span class="number">0.8</span>&#125;).invoke(<span class="string">&quot;随意挑选一个随机数，输出为一个整数&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="动态切换提示词"><a href="#动态切换提示词" class="headerlink" title="动态切换提示词"></a>动态切换提示词</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.runnables.hub <span class="keyword">import</span> HubRunnable</span><br><span class="line"></span><br><span class="line">prompt = HubRunnable(<span class="string">&quot;rlm/rag-prompt&quot;</span>).configurable_fields(</span><br><span class="line">    owner_repo_commit=ConfigurableField(</span><br><span class="line">        <span class="built_in">id</span>=<span class="string">&quot;hub_commit&quot;</span>,</span><br><span class="line">        name=<span class="string">&quot;Hub Commit&quot;</span>,</span><br><span class="line">        description=<span class="string">&quot;The Hub commit to pull from&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">prompt.invoke(&#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;context&quot;</span>: <span class="string">&quot;bar&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在运行时切换提示词</span></span><br><span class="line">prompt.with_config(configurable=&#123;<span class="string">&quot;hub_commit&quot;</span>: <span class="string">&quot;rlm/rag-prompt-llama&quot;</span>&#125;).invoke(&#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;context&quot;</span>: <span class="string">&quot;bar&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="为链增加记忆能力（短时记忆InMemoryHistory）"><a href="#为链增加记忆能力（短时记忆InMemoryHistory）" class="headerlink" title="为链增加记忆能力（短时记忆InMemoryHistory）"></a>为链增加记忆能力（短时记忆InMemoryHistory）</h2><ul>
<li>注意：简单的链的记忆天际可以使用v0.2的方式，复杂的官方推荐使用LangGraph</li>
<li>短时记忆：InMemoryHistory</li>
<li>长时记忆：RunnableWithMessageHistory</li>
</ul>
<h3 id="InMemoryHistory"><a href="#InMemoryHistory" class="headerlink" title="InMemoryHistory"></a>InMemoryHistory</h3><p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"><span class="keyword">from</span> langchain_core.chat_history <span class="keyword">import</span> BaseChatMessageHistory <span class="comment"># 导入聊天历史记录基类</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> BaseMessage, AIMessage <span class="comment"># 导入消息基类和具体消息类型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InMemoryHistory</span>(BaseChatMessageHistory, BaseModel):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    内存中实现聊天消息历史记录</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    messages: <span class="type">List</span>[BaseMessage] = Field(default_factory=<span class="built_in">list</span>) <span class="comment"># 使用空列表作为默认值存储消息</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_messages</span>(<span class="params">self, messages: <span class="type">List</span>[BaseMessage]</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        添加一组消息到存储中</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.messages.extend(messages)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">clear</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        清空存储中的所有消息</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.messages = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_messages</span>(<span class="params">self</span>) -&gt; <span class="type">List</span>[BaseMessage]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        获取存储中的所有消息</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.messages</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 这里我使用全局变量来存储聊天消息历史</span></span><br><span class="line"><span class="comment"># 这样可以更容易地检查它来查看底层结果</span></span><br><span class="line">store = &#123;&#125; <span class="comment"># 创建空字典用于存储不同会话的历史记录</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_by_session_id</span>(<span class="params">session_id: <span class="built_in">str</span></span>) -&gt; BaseChatMessageHistory:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    根据会话ID获取聊天历史记录,如果不存在则创建新的</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> session_id <span class="keyword">not</span> <span class="keyword">in</span> store:</span><br><span class="line">        store[session_id] = InMemoryHistory() <span class="comment"># 如果会话ID不存在，则创建一个新的历史记录</span></span><br><span class="line">    <span class="keyword">return</span> store[session_id] <span class="comment"># 返回对应的聊天历史记录</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取会话id为“1”的历史记录</span></span><br><span class="line">history = get_by_session_id(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"><span class="comment"># 添加一条AI消息到历史记录中</span></span><br><span class="line">history.add_messages([AIMessage(content=<span class="string">&quot;你好&quot;</span>)])</span><br><span class="line"><span class="comment"># 打印存储的所有历史记录</span></span><br><span class="line"><span class="built_in">print</span>(store) <span class="comment"># 将输出包含会话“1”的历史记录，其中有一条”你好“的AI消息</span></span><br></pre></td></tr></table></figure>
<h3 id="在链中增加短暂记忆"><a href="#在链中增加短暂记忆" class="headerlink" title="在链中增加短暂记忆"></a>在链中增加短暂记忆</h3><p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> ChatPromptTemplate, MessagesPlaceholder <span class="comment"># 倒入聊天提示模板和消息占位符</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables.history <span class="keyword">import</span> RunnableWithMessageHistory <span class="comment"># 导入带历史记录的可运行组件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建聊天提示模板，包含系统提示、历史记录和用户问题</span></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个擅长&#123;ability&#125;的助手&quot;</span>), <span class="comment"># 系统提示，使用ability变量定义助手专长</span></span><br><span class="line">        MessagesPlaceholder(variable_name=<span class="string">&quot;history&quot;</span>), <span class="comment"># 放置历史消息的占位符</span></span><br><span class="line">        (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;question&#125;&quot;</span>), <span class="comment"># 用户问题的占位符</span></span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将提示模板与大模型连接成一个链</span></span><br><span class="line">chain = prompt | llm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建带有消息历史功能的可运行链</span></span><br><span class="line">chain_with_history = RunnableWithMessageHistory(</span><br><span class="line">    chain, <span class="comment"># 基础链</span></span><br><span class="line">    get_by_session_id, <span class="comment"># 使用上一个示例中的get_by_session_id函数获取历史记录</span></span><br><span class="line">    input_messages_key=<span class="string">&quot;question&quot;</span>, <span class="comment"># 输入中代表用户问题的键</span></span><br><span class="line">    history_messages_key=<span class="string">&quot;history&quot;</span>, <span class="comment"># 历史记录在输入中的键</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首次调用链，询问余弦的含义</span></span><br><span class="line">res = chain_with_history.invoke(</span><br><span class="line">    &#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;什么是余弦？&quot;</span>, <span class="string">&quot;ability&quot;</span>: <span class="string">&quot;解释科学概念&quot;</span>&#125;, </span><br><span class="line">    config=&#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;session_id&quot;</span>: <span class="string">&quot;foo&quot;</span>&#125;&#125; <span class="comment"># 配置会话ID为&quot;foo&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(res) <span class="comment"># 打印助手的回答</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;---------------&quot;</span>)</span><br><span class="line"><span class="comment"># 打印存储中的历史记录</span></span><br><span class="line"><span class="comment"># 此时应包含第一次对话的问题和回答</span></span><br><span class="line"><span class="built_in">print</span>(store)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二次调用，询问余弦的反函数</span></span><br><span class="line"><span class="comment"># 由于使用相同的会话ID，模型可以参考前一次对话的上下文</span></span><br><span class="line">res = chain_with_history.invoke(</span><br><span class="line">    &#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;余弦的反函数是什么？&quot;</span>, <span class="string">&quot;ability&quot;</span>: <span class="string">&quot;解释科学概念&quot;</span>&#125;, </span><br><span class="line">    config=&#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;session_id&quot;</span>: <span class="string">&quot;foo&quot;</span>&#125;&#125; <span class="comment"># 使用相同的会话ID</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(res) <span class="comment"># 打印助手的回答</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;---------------&quot;</span>)</span><br><span class="line"><span class="comment"># 打印存储中的历史记录</span></span><br><span class="line"><span class="comment"># 此时应包含两次对话的问题和回答</span></span><br><span class="line"><span class="built_in">print</span>(store)</span><br></pre></td></tr></table></figure>
<h3 id="增加用户与对话ID，精准控制记忆"><a href="#增加用户与对话ID，精准控制记忆" class="headerlink" title="增加用户与对话ID，精准控制记忆"></a>增加用户与对话ID，精准控制记忆</h3><p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> ( ConfigurableFieldSpec )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建空字典用于存储不同用户和对话的历史记录</span></span><br><span class="line">store = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_session_history</span>(<span class="params">user_id: <span class="built_in">str</span>, session_id: <span class="built_in">str</span></span>) -&gt; BaseChatMessageHistory:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    根据用户ID和会话ID获取聊天历史记录</span></span><br><span class="line"><span class="string">    如果不存在则创建新的历史记录对象</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        user_id: 用户的唯一标识符</span></span><br><span class="line"><span class="string">        session_id: 对话的唯一标识符</span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        对应的聊天历史记录对象</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span>(user_id, session_id) <span class="keyword">not</span> <span class="keyword">in</span> store:</span><br><span class="line">        store[(user_id, session_id)] = InMemoryHistory() <span class="comment"># 如果历史记录不存在，则创建一个新的</span></span><br><span class="line">    <span class="keyword">return</span> store[(user_id, session_id)] <span class="comment"># 返回对应的聊天历史记录</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建聊天提示模板，包含系统提示、历史记录和用户问题</span></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个擅长&#123;ability&#125;的助手&quot;</span>), <span class="comment"># 系统提示，使用ability变量定义助手专长</span></span><br><span class="line">        MessagesPlaceholder(variable_name=<span class="string">&quot;history&quot;</span>), <span class="comment"># 放置历史消息的占位符</span></span><br><span class="line">        (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;question&#125;&quot;</span>), <span class="comment"># 用户问题的占位符</span></span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将提示模板与大模型连接成一个链</span></span><br><span class="line">chain = prompt | llm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建带有消息历史功能的可运行链,支持用户ID和对话ID配置</span></span><br><span class="line">_with_message_history = RunnableWithMessageHistory(</span><br><span class="line">    chain, <span class="comment"># 基础链</span></span><br><span class="line">    get_session_history=get_session_history, <span class="comment"># 获取历史记录的函数</span></span><br><span class="line">    input_messages_key=<span class="string">&quot;question&quot;</span>, <span class="comment"># 输入中代表用户问题的键</span></span><br><span class="line">    history_messages_key=<span class="string">&quot;history&quot;</span>, <span class="comment"># 历史记录在输入中的键</span></span><br><span class="line">    history_factory_config=[</span><br><span class="line">        ConfigurableFieldSpec(</span><br><span class="line">            <span class="built_in">id</span>=<span class="string">&quot;user_id&quot;</span>, <span class="comment"># 配置字段ID</span></span><br><span class="line">            annotation=<span class="built_in">str</span>, <span class="comment"># 字段类型注解</span></span><br><span class="line">            name=<span class="string">&quot;用户ID&quot;</span>, <span class="comment"># 字段名称</span></span><br><span class="line">            description=<span class="string">&quot;用户的唯一标识符&quot;</span>, <span class="comment"># 字段描述</span></span><br><span class="line">            default=<span class="string">&quot;&quot;</span>, <span class="comment"># 默认值</span></span><br><span class="line">            is_shared=<span class="literal">True</span>, <span class="comment"># 是否在多个调用间共享 </span></span><br><span class="line">        ),</span><br><span class="line">        ConfigurableFieldSpec(</span><br><span class="line">            <span class="built_in">id</span>=<span class="string">&quot;session_id&quot;</span>,</span><br><span class="line">            annotation=<span class="built_in">str</span>,</span><br><span class="line">            name=<span class="string">&quot;会话ID&quot;</span>,</span><br><span class="line">            description=<span class="string">&quot;对话的唯一标识符&quot;</span>,</span><br><span class="line">            default=<span class="string">&quot;&quot;</span>,</span><br><span class="line">            is_shared=<span class="literal">True</span>,</span><br><span class="line">        ),</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用链，询问余弦的含义</span></span><br><span class="line"><span class="comment"># 指定用户ID为“123”，对话ID为“1”</span></span><br><span class="line">res = _with_message_history.invoke(</span><br><span class="line">    &#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;什么是余弦？&quot;</span>, <span class="string">&quot;ability&quot;</span>: <span class="string">&quot;解释科学概念&quot;</span>&#125;, </span><br><span class="line">    config=&#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;user_id&quot;</span>: <span class="string">&quot;123&quot;</span>, <span class="string">&quot;session_id&quot;</span>: <span class="string">&quot;1&quot;</span>&#125;&#125; <span class="comment"># 配置用户ID和会话ID</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(res) <span class="comment"># 打印助手的回答</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;---------------&quot;</span>)</span><br><span class="line"><span class="comment"># 打印存储中的历史记录</span></span><br><span class="line"><span class="built_in">print</span>(store)</span><br></pre></td></tr></table></figure>
<h2 id="使用Redis构建长期记忆"><a href="#使用Redis构建长期记忆" class="headerlink" title="使用Redis构建长期记忆"></a>使用Redis构建长期记忆</h2><ul>
<li>安装redis</li>
<li>运行redis服务</li>
<li>配置长期记忆</li>
</ul>
<p>推荐安装redis-stack</p>
<p>安装依赖：<code>pip install langchain-redis redis</code></p>
<p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_redis <span class="keyword">import</span> RedisChatMessageHistory</span><br><span class="line"></span><br><span class="line">REDIS_URL = <span class="string">&quot;redis://localhost:6379&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Connecting to Redis at: <span class="subst">&#123;REDIS_URL&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 Redis 聊天消息历史记录</span></span><br><span class="line"><span class="comment"># 使用 Redis 存储聊天历史，需要提供会话 ID 和 Redis 连接 URL</span></span><br><span class="line">history = RedisChatMessageHistory(session_id=<span class="string">&quot;user_123&quot;</span>, redis_url=REDIS_URL)</span><br><span class="line">history.clear()  <span class="comment"># 首先清空历史记录</span></span><br><span class="line"><span class="comment"># 向历史记录中添加消息</span></span><br><span class="line">history.add_user_message(<span class="string">&quot;你好，AI助手！2222&quot;</span>)  <span class="comment"># 添加用户消息</span></span><br><span class="line">history.add_ai_message(<span class="string">&quot;你好！我今天能为你提供什么帮助？222&quot;</span>)  <span class="comment"># 添加AI回复消息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检索并显示历史消息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;聊天历史：&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> message <span class="keyword">in</span> history.messages:</span><br><span class="line">    <span class="comment"># 打印每条消息的类型和内容</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="built_in">type</span>(message).__name__&#125;</span>: <span class="subst">&#123;message.content&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="使用LCEL来自定义路由链"><a href="#使用LCEL来自定义路由链" class="headerlink" title="使用LCEL来自定义路由链"></a>使用LCEL来自定义路由链</h2><p>通过自定义路由链和回退机制实现智能请求分类与模型调用容错，提升应用稳定性与灵活性。</p>
<p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">api_base = os.getenv(<span class="string">&quot;OPENAI_API_BASE&quot;</span>)</span><br><span class="line">api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=<span class="number">0.0</span>,</span><br><span class="line">    base_url=api_base,</span><br><span class="line">    api_key=api_key</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser  <span class="comment"># 导入字符串输出解析器</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> PromptTemplate  <span class="comment"># 导入提示模板</span></span><br><span class="line"><span class="comment"># 导入RunnableLambda用于创建可运行的函数链</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableLambda</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建分类链 - 用于确定问题类型</span></span><br><span class="line">chain = (</span><br><span class="line">    <span class="comment"># 创建提示模板，要求模型将问题分类为LangChain、Anthropic或Other</span></span><br><span class="line">    PromptTemplate.from_template(</span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据下面的用户问题，将其分类为 `LangChain`、`Anthropic` 或 `Other`。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">请只回复一个词作为答案。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;question&gt;</span></span><br><span class="line"><span class="string">&#123;question&#125;</span></span><br><span class="line"><span class="string">&lt;/question&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">分类结果:&quot;&quot;&quot;</span></span><br><span class="line">    )</span><br><span class="line">    | llm  <span class="comment"># 将提示发送给大模型</span></span><br><span class="line">    | StrOutputParser()  <span class="comment"># 解析模型的输出为纯文本</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建LangChain专家链 - 模拟Harrison Chase(LangChain创始人)的回答风格</span></span><br><span class="line">langchain_chain = PromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;&quot;&quot;你将扮演一位LangChain专家。请以他的视角回答问题。 \</span></span><br><span class="line"><span class="string">你的回答必须以&quot;正如Harrison Chase告诉我的&quot;开头，否则你会受到惩罚。 \</span></span><br><span class="line"><span class="string">请回答以下问题:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">问题: &#123;question&#125;</span></span><br><span class="line"><span class="string">回答:&quot;&quot;&quot;</span></span><br><span class="line">) | llm  <span class="comment"># 将提示发送给</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Anthropic专家链 - 模拟Dario Amodei(Anthropic创始人)的回答风格</span></span><br><span class="line">anthropic_chain = PromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;&quot;&quot;你将扮演一位一位Anthropic专家。请以他的视角回答问题。 \</span></span><br><span class="line"><span class="string">你的回答必须以&quot;正如Dario Amodei告诉我的&quot;开头，否则你会受到惩罚。 \</span></span><br><span class="line"><span class="string">请回答以下问题:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">问题: &#123;question&#125;</span></span><br><span class="line"><span class="string">回答:&quot;&quot;&quot;</span></span><br><span class="line">) | llm  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建通用回答链 - 用于处理其他类型的问题</span></span><br><span class="line">general_chain = PromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;&quot;&quot;请回答以下问题:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">问题: &#123;question&#125;</span></span><br><span class="line"><span class="string">回答:&quot;&quot;&quot;</span></span><br><span class="line">) | llm </span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义路由函数 - 根据问题分类结果选择合适的回答链</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">route</span>(<span class="params">info</span>):</span><br><span class="line">    <span class="built_in">print</span>(info)   <span class="comment"># 打印分类结果</span></span><br><span class="line">    <span class="comment"># 根据分类结果选择相应的专家链</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;anthropic&quot;</span> <span class="keyword">in</span> info[<span class="string">&quot;topic&quot;</span>].lower():  <span class="comment"># 如果问题与Anthropic相关</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;claude&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> anthropic_chain  <span class="comment"># 使用Anthropic专家链</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&quot;langchain&quot;</span> <span class="keyword">in</span> info[<span class="string">&quot;topic&quot;</span>].lower():  <span class="comment"># 如果问题与LangChain相关</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;langchain&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> langchain_chain  <span class="comment"># 使用LangChain专家链</span></span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 其他类型的问题</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;general&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> general_chain <span class="comment"># 使用通用回答链</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建完整的处理链</span></span><br><span class="line"><span class="comment"># 1. 首先将问题分类并保留原始问题</span></span><br><span class="line"><span class="comment"># 2. 然后根据分类结果路由到相应的专家链处理</span></span><br><span class="line">full_chain = &#123;<span class="string">&quot;topic&quot;</span>: chain, <span class="string">&quot;question&quot;</span>: <span class="keyword">lambda</span> x: x[<span class="string">&quot;question&quot;</span>]&#125; | RunnableLambda(route)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用完整链处理用户问题</span></span><br><span class="line"><span class="comment"># 这个问题会被分类为Anthropic相关，然后由anthropic_chain处理</span></span><br><span class="line">full_chain.invoke(&#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;我该如何使用langchain?&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="链的回退机制"><a href="#链的回退机制" class="headerlink" title="链的回退机制"></a>链的回退机制</h2><p>调用大模型超过速率限制如何处理</p>
<p>示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">from</span> unittest.mock <span class="keyword">import</span> patch  <span class="comment"># 导入mock库，用于模拟函数行为</span></span><br><span class="line"><span class="keyword">from</span> langchain_anthropic <span class="keyword">import</span> ChatAnthropic  <span class="comment"># 导入Anthropic的语言模型接口</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI  <span class="comment"># 导入OpenAI的语言模型接口</span></span><br><span class="line"><span class="keyword">import</span> httpx  <span class="comment"># HTTP客户端库</span></span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> RateLimitError  <span class="comment"># OpenAI的速率限制错误类</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模拟HTTP请求和响应对象，用于构造模拟的API错误</span></span><br><span class="line">request = httpx.Request(<span class="string">&quot;GET&quot;</span>, <span class="string">&quot;/&quot;</span>)  <span class="comment"># 创建一个GET请求</span></span><br><span class="line">response = httpx.Response(<span class="number">200</span>, request=request)  <span class="comment"># 创建一个状态码为200的响应</span></span><br><span class="line"><span class="comment"># 创建一个OpenAI速率限制错误对象，用于模拟API调用超出速率限制的情况</span></span><br><span class="line">error = RateLimitError(<span class="string">&quot;rate limit&quot;</span>, response=response, body=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化OpenAI模型</span></span><br><span class="line"><span class="comment"># 注意：设置max_retries = 0是为了避免在遇到速率限制等错误时自动重试</span></span><br><span class="line">openai_llm = ChatOpenAI(</span><br><span class="line">    model=<span class="string">&quot;gpt-4&quot;</span>,  <span class="comment"># 使用GPT-4模型</span></span><br><span class="line">    temperature=<span class="number">0</span>,  <span class="comment"># 设置温度为0，使输出更确定性</span></span><br><span class="line">    api_key=os.environ.get(<span class="string">&quot;OPENAI_API_KEY&quot;</span>),  <span class="comment"># 从环境变量获取API密钥</span></span><br><span class="line">    base_url=os.environ.get(<span class="string">&quot;OPENAI_API_BASE&quot;</span>),  <span class="comment"># 从环境变量获取基础URL</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化Anthropic模型作为备用选项</span></span><br><span class="line">anthropic_llm = ChatAnthropic(</span><br><span class="line">    model=<span class="string">&#x27;claude-3-5-sonnet-latest&#x27;</span>,  <span class="comment"># 使用Claude 3.5 Sonnet模型</span></span><br><span class="line">    api_key=os.environ.get(<span class="string">&quot;ANTHROPIC_API_KEY&quot;</span>),  <span class="comment"># 从环境变量获取API密钥</span></span><br><span class="line">    base_url=os.environ.get(<span class="string">&quot;ANTHROPIC_BASE_URL&quot;</span>),  <span class="comment"># 从环境变量获取基础URL</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建带有备用选项的语言模型</span></span><br><span class="line"><span class="comment"># 如果主模型(OpenAI)失败，将自动尝试使用备用模型(Anthropic)</span></span><br><span class="line">llm = openai_llm.with_fallbacks([anthropic_llm]) <span class="comment"># 可以设置多个模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果不设置回退机制，当API调用超量就会直接报错</span></span><br><span class="line"><span class="comment"># with patch(&quot;openai.resources.chat.completions.Completions.create&quot;, side_effect=error):</span></span><br><span class="line"><span class="comment">#    try:</span></span><br><span class="line"><span class="comment">#        print(llm.invoke(&quot;Why did the chicken cross the road?&quot;))</span></span><br><span class="line"><span class="comment">#    except RateLimitError:</span></span><br><span class="line"><span class="comment">#        print(&quot;Hit error&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试备用机制 - 使用中文问题</span></span><br><span class="line"><span class="comment"># 使用patch模拟OpenAI API调用失败（抛出速率限制错误）</span></span><br><span class="line"><span class="keyword">with</span> patch(<span class="string">&quot;openai.resources.chat.completions.Completions.create&quot;</span>, side_effect=error):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 尝试调用语言模型回答中文问题</span></span><br><span class="line">        <span class="comment"># 由于OpenAI被模拟为失败，应该自动切换到Anthropic模型</span></span><br><span class="line">        <span class="built_in">print</span>(llm.invoke(<span class="string">&quot;为什么程序员需要学会python?&quot;</span>))</span><br><span class="line">    <span class="keyword">except</span> RateLimitError:</span><br><span class="line">        <span class="comment"># 如果仍然遇到错误（备用机制失败），则打印错误信息</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Hit error&quot;</span>)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/LangChain/" rel="tag"># LangChain</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/02/05/ai/langchain/2-langchain-ChatModels/" rel="prev" title="LangChain组件ChatModels（磨平不同LLM的差异）">
      <i class="fa fa-chevron-left"></i> LangChain组件ChatModels（磨平不同LLM的差异）
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/02/05/ai/langchain/3-langchain-PromptTemple/" rel="next" title="LangChain组件PromptTemple">
      LangChain组件PromptTemple <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Runnable%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">Runnable介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFRunnable"><span class="nav-number">1.1.</span> <span class="nav-text">什么是Runnable</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8-Runnable"><span class="nav-number">1.2.</span> <span class="nav-text">为什么使用 Runnable</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Runnable-%E7%9A%84%E4%B8%BB%E8%A6%81%E6%96%B9%E6%B3%95"><span class="nav-number">1.3.</span> <span class="nav-text">Runnable 的主要方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%93%AA%E4%BA%9B%E7%BB%84%E4%BB%B6%E6%98%AF-Runnable"><span class="nav-number">1.4.</span> <span class="nav-text">哪些组件是 Runnable</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LCEL%E7%AE%80%E4%BB%8B"><span class="nav-number">2.</span> <span class="nav-text">LCEL简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LCEL%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="nav-number">3.</span> <span class="nav-text">LCEL的好处</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LCEL%E5%B0%81%E8%A3%85%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9"><span class="nav-number">4.</span> <span class="nav-text">LCEL封装核心内容</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LCEL%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">5.</span> <span class="nav-text">LCEL使用场景</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%93%BE%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="nav-number">6.</span> <span class="nav-text">链的基本使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%AE%A1%E9%81%93%E6%93%8D%E4%BD%9C%E7%AC%A6%E5%BF%AB%E9%80%9F%E7%94%9F%E6%88%90%E4%B8%80%E6%9D%A1%E9%93%BE"><span class="nav-number">6.1.</span> <span class="nav-text">使用管道操作符快速生成一条链</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%93%BE%E7%9A%84pipe%E6%96%B9%E5%BC%8F"><span class="nav-number">6.2.</span> <span class="nav-text">链的pipe方式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%93%BE%E7%9A%84%E6%B5%81%E5%BC%8F%E8%B0%83%E7%94%A8"><span class="nav-number">6.3.</span> <span class="nav-text">链的流式调用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5%E8%B0%83%E7%94%A8astream"><span class="nav-number">6.3.1.</span> <span class="nav-text">异步调用astream</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JSON%E6%B5%81%E8%BE%93%E5%87%BA"><span class="nav-number">6.3.2.</span> <span class="nav-text">JSON流输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8B%E4%BB%B6%E6%B5%81"><span class="nav-number">6.3.3.</span> <span class="nav-text">事件流</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8B%E4%BB%B6%E8%BF%87%E6%BB%A4-%E6%8C%89name"><span class="nav-number">6.3.3.1.</span> <span class="nav-text">事件过滤-按name</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8B%E4%BB%B6%E8%BF%87%E6%BB%A4-%E6%8C%89tag"><span class="nav-number">6.3.3.2.</span> <span class="nav-text">事件过滤 - 按tag</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8B%E4%BB%B6%E9%98%B6%E6%AE%B5%E8%BF%87%E6%BB%A4"><span class="nav-number">6.3.3.3.</span> <span class="nav-text">事件阶段过滤</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E8%BF%90%E8%A1%8C%E5%A4%9A%E6%9D%A1%E9%93%BE"><span class="nav-number">6.4.</span> <span class="nav-text">并行运行多条链</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%80%81%E7%89%88%E6%9C%AC%E7%9A%84chain%E8%BF%81%E7%A7%BB%E5%88%B0LCEL"><span class="nav-number">7.</span> <span class="nav-text">老版本的chain迁移到LCEL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B1-%E4%BB%8ELLMChain%E8%BF%81%E7%A7%BB"><span class="nav-number">7.1.</span> <span class="nav-text">示例1:从LLMChain迁移</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%9F%E5%BC%83%E7%9A%84%E7%94%A8%E6%B3%95"><span class="nav-number">7.1.1.</span> <span class="nav-text">废弃的用法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LCEL%E7%9A%84%E7%94%A8%E6%B3%95"><span class="nav-number">7.1.2.</span> <span class="nav-text">LCEL的用法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B2-%E4%BB%8EStuffDocumentsChain%E8%BF%81%E7%A7%BB"><span class="nav-number">7.2.</span> <span class="nav-text">示例2: 从StuffDocumentsChain迁移</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%9F%E5%BC%83%E7%9A%84%E7%94%A8%E6%B3%95-1"><span class="nav-number">7.2.1.</span> <span class="nav-text">废弃的用法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LCEL%E7%94%A8%E6%B3%95"><span class="nav-number">7.2.2.</span> <span class="nav-text">LCEL用法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%93%BE%E7%9A%84%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8"><span class="nav-number">8.</span> <span class="nav-text">链的高级应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8%E9%93%BE%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%87%BD%E6%95%B0"><span class="nav-number">8.1.</span> <span class="nav-text">在链中使用函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-chain%E5%BF%AB%E9%80%9F%E5%B0%86%E5%87%BD%E6%95%B0%E8%BD%AC%E4%B8%BA%E9%93%BE"><span class="nav-number">8.1.1.</span> <span class="nav-text">使用@chain快速将函数转为链</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%93%BE%E4%B8%AD%E4%BD%BF%E7%94%A8Lambda"><span class="nav-number">8.1.2.</span> <span class="nav-text">链中使用Lambda</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8%E9%93%BE%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89%E6%94%AF%E6%8C%81%E6%B5%81%E8%BE%93%E5%87%BA%E7%9A%84%E5%87%BD%E6%95%B0"><span class="nav-number">8.2.</span> <span class="nav-text">在链中自定义支持流输出的函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E9%93%BE%E6%94%AF%E6%8C%81%E6%B5%81%E8%B0%83%E7%94%A8"><span class="nav-number">8.2.1.</span> <span class="nav-text">一个简单的链支持流调用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A2%9E%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="nav-number">8.2.2.</span> <span class="nav-text">增加自定义函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%A9%E5%B1%95%EF%BC%9Ayeild%E4%B8%8Ereturn%E5%8C%BA%E5%88%AB"><span class="nav-number">8.2.2.1.</span> <span class="nav-text">扩展：yeild与return区别</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8RunnablePassthrough%E6%9D%A5%E4%BC%A0%E9%80%92%E5%80%BC"><span class="nav-number">8.3.</span> <span class="nav-text">使用RunnablePassthrough来传递值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%9C%A8%E8%BF%90%E8%A1%8C%E6%97%B6%E5%8A%A8%E6%80%81%E6%B7%BB%E5%8A%A0%E9%93%BE%E7%9A%84%E9%85%8D%E7%BD%AE"><span class="nav-number">8.4.</span> <span class="nav-text">如何在运行时动态添加链的配置</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E6%94%B9%E5%86%99%E6%A8%A1%E5%9E%8B%E6%B8%A9%E5%BA%A6"><span class="nav-number">8.4.1.</span> <span class="nav-text">动态改写模型温度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%88%87%E6%8D%A2%E6%8F%90%E7%A4%BA%E8%AF%8D"><span class="nav-number">8.4.2.</span> <span class="nav-text">动态切换提示词</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E9%93%BE%E5%A2%9E%E5%8A%A0%E8%AE%B0%E5%BF%86%E8%83%BD%E5%8A%9B%EF%BC%88%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86InMemoryHistory%EF%BC%89"><span class="nav-number">8.5.</span> <span class="nav-text">为链增加记忆能力（短时记忆InMemoryHistory）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#InMemoryHistory"><span class="nav-number">8.5.1.</span> <span class="nav-text">InMemoryHistory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E9%93%BE%E4%B8%AD%E5%A2%9E%E5%8A%A0%E7%9F%AD%E6%9A%82%E8%AE%B0%E5%BF%86"><span class="nav-number">8.5.2.</span> <span class="nav-text">在链中增加短暂记忆</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A2%9E%E5%8A%A0%E7%94%A8%E6%88%B7%E4%B8%8E%E5%AF%B9%E8%AF%9DID%EF%BC%8C%E7%B2%BE%E5%87%86%E6%8E%A7%E5%88%B6%E8%AE%B0%E5%BF%86"><span class="nav-number">8.5.3.</span> <span class="nav-text">增加用户与对话ID，精准控制记忆</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Redis%E6%9E%84%E5%BB%BA%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86"><span class="nav-number">8.6.</span> <span class="nav-text">使用Redis构建长期记忆</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8LCEL%E6%9D%A5%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B7%AF%E7%94%B1%E9%93%BE"><span class="nav-number">8.7.</span> <span class="nav-text">使用LCEL来自定义路由链</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%93%BE%E7%9A%84%E5%9B%9E%E9%80%80%E6%9C%BA%E5%88%B6"><span class="nav-number">8.8.</span> <span class="nav-text">链的回退机制</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jean Lv"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jean Lv</p>
  <div class="site-description" itemprop="description">涉猎的主要编程语言Java、Python, 领域软件测试、AI测试开发相关</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">159</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jean Lv</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  

  

    </div>
</body>
</html>
